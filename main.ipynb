{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731b06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string \n",
    "import random\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13815450",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "with ZipFile (\"depression.zip\",\"r\") as zip:\n",
    "    zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe504613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sentiment_tweets3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9629cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just had a real good moment i missssssssss him so much'\n",
      " 'is reading manga' ''\n",
      " 'need to send to my accountant tomorrow oddly i wasn even referring to my taxes those are supporting evidence though'\n",
      " 'add me on myspace myspace com lookthunder']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x):\n",
    "    x = x.lower()\n",
    "    x = x.encode(\"ascii\",\"ignore\").decode()\n",
    "    x = re.sub(\"https*\\S+\",\" \",x)\n",
    "    x = re.sub(\"@\\S+\",\" \",x)\n",
    "    x = re.sub(\"#\\S+\",\" \",x)\n",
    "    x = re.sub(\"\\'\\w+\",\"\",x)\n",
    "    x = re.sub(\"[%s]\" % re.escape(string.punctuation),\" \",x)\n",
    "    x = re.sub(\"\\w*\\d+\\w*\",\"\",x)\n",
    "    x = re.sub(\"\\s{2,}\",\" \",x)\n",
    "    return x\n",
    "\n",
    "temp = []\n",
    "data_to_list = df[\"message to examine\"].values.tolist()\n",
    "for i in range(len(data_to_list)):\n",
    "    temp.append(preprocess(data_to_list[i]))\n",
    "\n",
    "def tokenize(y):\n",
    "    for x in y:\n",
    "        yield(word_tokenize(str(x)))\n",
    "\n",
    "data_words = list(tokenize(temp))\n",
    "\n",
    "def detokenize(txt):\n",
    "    return TreebankWordDetokenizer().detokenize(txt)\n",
    "\n",
    "final_data = []\n",
    "for i in range(len(data_words)):\n",
    "    final_data.append(detokenize(data_words[i]))\n",
    "final_data = np.array(final_data)\n",
    "print(final_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00020419",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3177e11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  188   19   89]\n",
      " [   0    0    0 ...   11  489 6059]\n",
      " [   0    0    0 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ...  843    7  411]\n",
      " [   0    0    0 ...    7  126   80]\n",
      " [   0    0    0 ...    0    0    0]]\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         1052800   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,250,690\n",
      "Trainable params: 1,250,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = df[\"label (depression result)\"]\n",
    "\n",
    "max_words = 8225\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(final_data)\n",
    "sequences = tokenizer.texts_to_sequences(final_data)\n",
    "tweets = pad_sequences(sequences,maxlen=max_len)\n",
    "with open(\"tokenizer.pickle\",\"wb\") as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(tweets)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(tweets,labels,random_state=42)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.25,random_state=42)\n",
    "\n",
    "def model():\n",
    "    inputs = tf.keras.Input(shape=(None,),dtype=\"int32\")\n",
    "    x = layers.Embedding(max_words,128)(inputs)\n",
    "    x = layers.Bidirectional(layers.LSTM(64,return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    outputs = layers.Dense(2)(x)\n",
    "    model = tf.keras.Model(inputs,outputs)\n",
    "    return model\n",
    "\n",
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d21edb",
   "metadata": {},
   "source": [
    "### Train and evaluate the model on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e67af1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "182/182 [==============================] - 9s 28ms/step - loss: 0.1818 - accuracy: 0.9273 - val_loss: 0.0631 - val_accuracy: 0.9853\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.98527, saving model to gru.h5\n",
      "Epoch 2/5\n",
      "182/182 [==============================] - 4s 23ms/step - loss: 0.0280 - accuracy: 0.9928 - val_loss: 0.0659 - val_accuracy: 0.9826\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.98527\n",
      "Epoch 3/5\n",
      "182/182 [==============================] - 4s 23ms/step - loss: 0.0088 - accuracy: 0.9986 - val_loss: 0.0715 - val_accuracy: 0.9868\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.98527 to 0.98682, saving model to gru.h5\n",
      "Epoch 4/5\n",
      "182/182 [==============================] - 4s 23ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.0751 - val_accuracy: 0.9872\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.98682 to 0.98720, saving model to gru.h5\n",
      "Epoch 5/5\n",
      "182/182 [==============================] - 4s 23ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.0751 - val_accuracy: 0.9872\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.98720\n",
      "81/81 - 1s - loss: 0.0751 - accuracy: 0.9872\n",
      "\u001b[1m\n",
      "\n",
      "Accuracy: 98.72 %\n",
      "Loss: 7.51 %\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    model.compile(tf.keras.optimizers.Adam(),tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=[\"accuracy\"])\n",
    "    checkpoint = ModelCheckpoint(\"gru.h5\",monitor=\"val_accuracy\",verbose=1,save_best_only=True,save_weights_only=False)\n",
    "    model.fit(x_train,y_train,batch_size=32,epochs=5,validation_data=(x_test,y_test),callbacks=[checkpoint])\n",
    "    best = tf.keras.models.load_model(\"gru.h5\")\n",
    "    loss,acc = best.evaluate(x_test,y_test,verbose=2)\n",
    "    print(\"\\033[1m\")\n",
    "    print(\"\\nAccuracy: {:.2f} %\".format(100*acc))\n",
    "    print(\"Loss: {:.2f} %\".format(100*loss))\n",
    "    print(\"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677952ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
